{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3179493e-6706-439f-84d4-752cbacf965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUSINESS LOGO DOMAIN CLASSIFICATION\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "print(\"BUSINESS LOGO DOMAIN CLASSIFICATION\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42087470-cdfd-4041-b307-6ffe11fb9589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 10000 samples\n",
      " Columns: ['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at', 'updated_at', 'legal_name', 'roles', 'domain', 'homepage_url', 'region', 'city', 'address', 'postal_code', 'status', 'short_description', 'num_funding_rounds', 'total_funding_usd', 'total_funding', 'total_funding_currency_code', 'founded_on', 'last_funding_on', 'closed_on', 'employee_count', 'email', 'phone', 'facebook_url', 'linkedin_url', 'twitter_url', 'state_code', 'logo_url', 'country_code', 'category_groups_list', 'category_list', 'new_logo_url']\n",
      " Found 9998 logo files\n",
      "✓ Matched 9990 logos (99.9%)\n",
      "✓ Total unique categories (multi-label): 47\n",
      "✓ Total samples (logos with at least 1 matched file): 9990\n",
      "\n",
      "Category frequency distribution (based on individual categories):\n",
      "  Categories with 1 sample:      2861\n",
      "  Categories with 2-4 samples:   819\n",
      "  Categories with 5-9 samples:   200\n",
      "  Categories with 10-19 samples: 75\n",
      "  Categories with 20+ samples:   51\n",
      "\n",
      "Top 20 most common categories:\n",
      "  1. [5076] Software\n",
      "  2. [2378] Financial Services\n",
      "  3. [2301] Information Technology\n",
      "  4. [2043] Science and Engineering\n",
      "  5. [2007] Internet Services\n",
      "  6. [1893] Data and Analytics\n",
      "  7. [1720] Health Care\n",
      "  8. [1455] Hardware\n",
      "  9. [1373] Commerce and Shopping\n",
      " 10. [1373] Other\n",
      " 11. [1085] Artificial Intelligence\n",
      " 12. [1073] Mobile\n",
      " 13. [ 956] Lending and Investments\n",
      " 14. [ 903] Transportation\n",
      " 15. [ 863] Media and Entertainment\n",
      " 16. [ 810] Professional Services\n",
      " 17. [ 778] Sales and Marketing\n",
      " 18. [ 778] Biotechnology\n",
      " 19. [ 753] Apps\n",
      " 20. [ 688] Payments\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\BCIT\\data_science\\project\\LogoPredictor\\top10k_logos.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "logo_dir = r\"C:\\BCIT\\data_science\\project\\logo_images\"\n",
    "\n",
    "print(f\" Loaded {len(df)} samples\")\n",
    "print(f\" Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Get list of available logo files\n",
    "logo_files = os.listdir(logo_dir)\n",
    "logo_files_set = set(logo_files)\n",
    "print(f\" Found {len(logo_files)} logo files\")\n",
    "\n",
    "def clean_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).lower().replace(' ', '_').replace('-', '_')\n",
    "    return ''.join(c for c in name if c.isalnum() or c == '_')\n",
    "\n",
    "df['clean_name'] = df['name'].apply(clean_name)\n",
    "\n",
    "\n",
    "matched_rows = []\n",
    "for logo_file in logo_files:\n",
    "    logo_name_raw = os.path.splitext(logo_file)[0]\n",
    "\n",
    "    # Many files may be like \"12345_company_name\", so drop leading ID part\n",
    "    if '_' in logo_name_raw:\n",
    "        logo_name = '_'.join(logo_name_raw.split('_', 1)[1:])\n",
    "    else:\n",
    "        logo_name = logo_name_raw\n",
    "    \n",
    "    logo_cleaned = clean_name(logo_name)\n",
    "    \n",
    "    # Find matching row in df\n",
    "    matches = df[df['clean_name'] == logo_cleaned]\n",
    "    if len(matches) > 0:\n",
    "        row = matches.iloc[0].copy()\n",
    "        row['logo_filename'] = logo_file\n",
    "        matched_rows.append(row)\n",
    "\n",
    "df_matched = pd.DataFrame(matched_rows)\n",
    "print(f\"✓ Matched {len(df_matched)} logos ({len(df_matched)/len(logo_files)*100:.1f}%)\")\n",
    "\n",
    "def parse_categories(x):\n",
    "    \"\"\"\n",
    "    Convert the raw category_groups_list string into a Python list.\n",
    "    Handles:\n",
    "      - \"['Software', 'AI']\"   (Python-list style)\n",
    "      - \"Software, AI\"        (comma-separated)\n",
    "      - NaNs                  (returns [])\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    \n",
    "    x = str(x).strip()\n",
    "    \n",
    "    # Try to parse as Python list: \"['A', 'B']\"\n",
    "    if x.startswith('[') and x.endswith(']'):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            # Ensure list of strings\n",
    "            return [str(c).strip() for c in parsed]\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Fallback: comma-separated string\n",
    "    return [c.strip() for c in x.split(',') if c.strip()]\n",
    "\n",
    "df_matched[\"parsed_categories\"] = df_matched[\"category_groups_list\"].apply(parse_categories)\n",
    "\n",
    "cat_counter = Counter()\n",
    "for cats in df_matched[\"parsed_categories\"]:\n",
    "    cat_counter.update(cats)\n",
    "\n",
    "# Convert to Series for convenience\n",
    "category_counts = pd.Series(cat_counter).sort_values(ascending=False)\n",
    "\n",
    "print(f\"✓ Total unique categories (multi-label): {len(category_counts)}\")\n",
    "print(f\"✓ Total samples (logos with at least 1 matched file): {len(df_matched)}\")\n",
    "\n",
    "# Distribution of category frequencies\n",
    "print(\"\\nCategory frequency distribution (based on individual categories):\")\n",
    "freq_values = df_matched['category_groups_list'].value_counts()\n",
    "print(f\"  Categories with 1 sample:      {(freq_values == 1).sum()}\")\n",
    "print(f\"  Categories with 2-4 samples:   {((freq_values >= 2) & (freq_values <= 4)).sum()}\")\n",
    "print(f\"  Categories with 5-9 samples:   {((freq_values >= 5) & (freq_values <= 9)).sum()}\")\n",
    "print(f\"  Categories with 10-19 samples: {((freq_values >= 10) & (freq_values < 20)).sum()}\")\n",
    "print(f\"  Categories with 20+ samples:   {(freq_values >= 20).sum()}\")\n",
    "\n",
    "print(\"\\nTop 20 most common categories:\")\n",
    "for i, (cat, count) in enumerate(category_counts.head(20).items(), 1):\n",
    "    short_cat = cat[:70] + \"...\" if len(cat) > 70 else cat\n",
    "    print(f\"{i:3d}. [{count:4d}] {short_cat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "240508b3-4654-4781-896f-4e5b0f982ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing filtered logos...\n",
      "  200/9990\n",
      "  400/9990\n",
      "  600/9990\n",
      "  800/9990\n",
      "  1000/9990\n",
      "  1200/9990\n",
      "  1400/9990\n",
      "  1600/9990\n",
      "  1800/9990\n",
      "  2000/9990\n",
      "  2200/9990\n",
      "  2400/9990\n",
      "  2600/9990\n",
      "  2800/9990\n",
      "  3000/9990\n",
      "  3200/9990\n",
      "  3400/9990\n",
      "  3600/9990\n",
      "  3800/9990\n",
      "  4000/9990\n",
      "  4200/9990\n",
      "  4400/9990\n",
      "  4600/9990\n",
      "  4800/9990\n",
      "  5000/9990\n",
      "  5200/9990\n",
      "  5400/9990\n",
      "  5600/9990\n",
      "  5800/9990\n",
      "  6000/9990\n",
      "  6200/9990\n",
      "  6400/9990\n",
      "  6600/9990\n",
      "  6800/9990\n",
      "  7000/9990\n",
      "  7200/9990\n",
      "  7400/9990\n",
      "  7600/9990\n",
      "  7800/9990\n",
      "  8000/9990\n",
      "  8200/9990\n",
      "  8400/9990\n",
      "  8600/9990\n",
      "  8800/9990\n",
      "  9000/9990\n",
      "  9200/9990\n",
      "  9400/9990\n",
      "  9600/9990\n",
      "  9800/9990\n",
      " Extracted: 9943 samples (failed: 47)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "failed = 0\n",
    "\n",
    "print(\"Processing filtered logos...\")\n",
    "with torch.no_grad():\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        logo_path = os.path.join(logo_dir, row['logo_filename'])\n",
    "        try:\n",
    "            img = Image.open(logo_path).convert('RGB')\n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "            feat = resnet(img_tensor).squeeze().cpu().numpy()\n",
    "            \n",
    "            features_list.append(feat)\n",
    "            labels_list.append(row['category_groups_list'])\n",
    "            \n",
    "            if (len(features_list)) % 200 == 0:\n",
    "                print(f\"  {len(features_list)}/{len(df_filtered)}\")\n",
    "        except:\n",
    "            failed += 1\n",
    "\n",
    "X = np.array(features_list)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "print(f\" Extracted: {len(X)} samples (failed: {failed})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e0ce17-aaf6-4971-89c0-a7118986fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3994 category combinations retained\n",
      "  Samples per category: min=1, max=444, mean=2.5\n",
      "\n",
      "Category distribution in encoded data:\n",
      "  Categories with 1 sample: 2856\n",
      "  Categories with 2+ samples: 1138\n",
      "\n",
      "Split strategy:\n",
      "  Single-sample categories → all to training set: 2856 samples\n",
      "  Multi-sample categories → stratified split: 7087 samples\n",
      "\n",
      " Train: 8525, Test: 1418\n"
     ]
    }
   ],
   "source": [
    "#Preparing data\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\" {len(le.classes_)} category combinations retained\")\n",
    "print(f\"  Samples per category: min={np.min(np.bincount(y_encoded))}, max={np.max(np.bincount(y_encoded))}, mean={np.mean(np.bincount(y_encoded)):.1f}\")\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle single-sample categories\n",
    "category_counts_encoded = pd.Series(y_encoded).value_counts()\n",
    "print(f\"\\nCategory distribution in encoded data:\")\n",
    "print(f\"  Categories with 1 sample: {(category_counts_encoded == 1).sum()}\")\n",
    "print(f\"  Categories with 2+ samples: {(category_counts_encoded >= 2).sum()}\")\n",
    "\n",
    "# Separate single-sample and multi-sample categories\n",
    "single_sample_cats = category_counts_encoded[category_counts_encoded == 1].index\n",
    "multi_sample_cats = category_counts_encoded[category_counts_encoded >= 2].index\n",
    "\n",
    "single_mask = np.isin(y_encoded, single_sample_cats)\n",
    "multi_mask = ~single_mask\n",
    "\n",
    "print(f\"\\nSplit strategy:\")\n",
    "print(f\"  Single-sample categories → all to training set: {single_mask.sum()} samples\")\n",
    "print(f\"  Multi-sample categories → stratified split: {multi_mask.sum()} samples\")\n",
    "\n",
    "# Split only multi-sample data with stratification\n",
    "if multi_mask.sum() > 0:\n",
    "    X_multi = X_scaled[multi_mask]\n",
    "    y_multi = y_encoded[multi_mask]\n",
    "    \n",
    "    X_train_multi, X_test, y_train_multi, y_test = train_test_split(\n",
    "        X_multi, y_multi,\n",
    "        test_size=0.20,\n",
    "        random_state=42,\n",
    "        stratify=y_multi\n",
    "    )\n",
    "    \n",
    "    # Add single-sample data to training set\n",
    "    if single_mask.sum() > 0:\n",
    "        X_single = X_scaled[single_mask]\n",
    "        y_single = y_encoded[single_mask]\n",
    "        \n",
    "        X_train = np.vstack([X_train_multi, X_single])\n",
    "        y_train = np.concatenate([y_train_multi, y_single])\n",
    "    else:\n",
    "        X_train = X_train_multi\n",
    "        y_train = y_train_multi\n",
    "else:\n",
    "    # All categories have single samples - cannot stratify\n",
    "    print(\"    Cannot perform stratified split - using random split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded,\n",
    "        test_size=0.20,\n",
    "        random_state=42,\n",
    "        stratify=None\n",
    "    )\n",
    "\n",
    "print(f\"\\n Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce5d677-ae08-4abb-bf66-78ce3d7754ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "#Train Model\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=25,\n",
    "    min_samples_split=8,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2711ab6-2ce4-4ad9-a98f-e6b93660f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "Training Accuracy: 0.2256 (22.56%)\n",
      "Testing Accuracy:  0.0007 (0.07%)\n",
      "F1-Score:          0.0000\n",
      "Overfitting Gap:   22.5%\n",
      "\n",
      "Baseline:\n",
      "  Random: 0.0003 (0.03%)\n",
      "  Model:  0.0007 (0.07%)\n",
      "  Improvement: 2.8x\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION REPORT\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 1314, does not match size of target_names, 3994. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLASSIFICATION REPORT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2970\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2964\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2965\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2966\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2967\u001b[0m             )\n\u001b[0;32m   2968\u001b[0m         )\n\u001b[0;32m   2969\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2972\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2973\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2974\u001b[0m         )\n\u001b[0;32m   2975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2976\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 1314, does not match size of target_names, 3994. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"F1-Score:          {test_f1:.4f}\")\n",
    "print(f\"Overfitting Gap:   {(train_acc-test_acc)*100:.1f}%\")\n",
    "\n",
    "random_guess = 1.0 / len(le.classes_)\n",
    "print(f\"\\nBaseline:\")\n",
    "print(f\"  Random: {random_guess:.4f} ({random_guess*100:.2f}%)\")\n",
    "print(f\"  Model:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"  Improvement: {test_acc/random_guess:.1f}x\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=le.classes_, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ca2d0-6dea-41db-b3f6-0e61e1d3c8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
